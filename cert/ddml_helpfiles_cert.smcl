{smcl}
{com}{sf}{ul off}{txt}{.-}
      name:  {res}<unnamed>
       {txt}log:  {res}/Users/kahrens/MyProjects/ddml/cert/ddml_helpfiles_cert.smcl
  {txt}log type:  {res}smcl
 {txt}opened on:  {res}31 Aug 2024, 14:03:06
{txt}
{com}. which ddml
{txt}/Users/kahrens/MyProjects/ddml/ddml.ado
{res}*! ddml v1.4.4
*! last edited: 30aug2024
*! authors: aa/ms
{txt}
{com}. mata: whichddml()
{res}  
  Mata library for ddml and related programs,
  compiled 29 Aug 2024 under Stata 15.1 born 03 Feb 2020.
  authors AA/MS
{txt}
{com}. which qddml
{txt}/Users/kahrens/MyProjects/ddml/qddml.ado
{res}*! ddml v1.4.4
*! last edited: 30aug2024
*! authors: aa/ms
{txt}
{com}. which crossfit
{txt}/Users/kahrens/MyProjects/ddml/crossfit.ado
{res}*! ddml v1.4.4
*! last edited: 30aug2024
*! authors: aa/ms
{txt}
{com}. which pystacked
{txt}/Users/kahrens/Library/Application Support/Stata/ado/plus/p/pystacked.ado
{res}*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms
{txt}
{com}. log close
      {txt}name:  {res}<unnamed>
       {txt}log:  {res}/Users/kahrens/MyProjects/ddml/cert/ddml_helpfiles_cert.smcl
  {txt}log type:  {res}smcl
 {txt}closed on:  {res}31 Aug 2024, 14:03:06
{txt}{.-}
{smcl}
{txt}{sf}{ul off}{smcl}
{com}{sf}{ul off}{txt}{.-}
      name:  {res}<unnamed>
       {txt}log:  {res}/Users/kahrens/MyProjects/ddml/cert/ddml_helpfiles_cert.smcl
  {txt}log type:  {res}smcl
 {txt}opened on:  {res}31 Aug 2024, 14:03:06

Help file: crossfit.sthlp

      {txt}name:  {res}<unnamed>
       {txt}log:  {res}/Users/kahrens/MyProjects/ddml/cert/ddml_helpfiles_cert.smcl
  {txt}log type:  {res}smcl
 {txt}closed on:  {res}31 Aug 2024, 14:03:06
{txt}{.-}
{smcl}
{txt}{sf}{ul off}{smcl}
{txt}{sf}{ul off}{res}{txt}{smcl}
{* *! version 30aug2024}{...}
{viewerjumpto "Syntax" "crossfit##syntax"}{...}
{viewerjumpto "Summary" "crossfit##summary"}{...}
{viewerjumpto "Compatible programs" "crossfit##compatibility"}{...}
{viewerjumpto "Examples" "crossfit##examples"}{...}
{viewerjumpto "Saved results" "crossfit##results"}{...}
{viewerjumpto "References" "crossfit##references"}{...}
{viewerjumpto "Authors" "crossfit##authors"}{...}
{vieweralsosee "ddml main page" "ddml"}{...}
{vieweralsosee "ddml crossfit" "ddml crossfit"}{...}
{vieweralsosee "ddml stacking" "ddml stacking"}{...}
{vieweralsosee "Other" "crossfit##also_see"}{...}
{hline}
{cmd:help crossfit}{right: v1.4.4}
{hline}


{title:crossfit - Stata program for cross-fitting}

{pstd}
{opt crossfit} fits a supervised machine learner on K-1 folds
and returns the out-of-sample predicted values for the holdout fold.
This is done iteratively to obtain out-of-sample ("cross-fitted") fitted values for the whole sample.

{pstd}
{opt crossfit} is an auxiliary program that is internally used by 
{help ddml} and {help qddml}, but can be used for other purposes.


{marker syntax}{...}
{title:Syntax}

{p 8 14 2}
{cmd:crossfit} , 
{opt estring(string)}
{opt g:enerate(stubname)}
[{opt kfolds(integer)}
{opt foldvar(varlist)}
{opt norandom}
{opt reps(integer)}
{opt vtype(string)}]

{synoptset 20}{...}
{synopthdr:Option}
{synoptline}
{synopt:{opt estring(string)}}
An estimation string, e.g. "reg y x1 x2", that will be 
repeatedly invoked. See note on compatible programs 
{help ddml##compatibility:here}.
{p_end}
{synopt:{opt g:enerate(stubname)}}
Name of the new variable to be created;
the resample number is appended to the end of the variable name.
Note that if the variable (including the resample number) already exists, it is overwritten.
{p_end}
{synopt:{opt kfolds(integer)}}
Number of randomly drawn folds; ignored if {opt foldvar(varlist)} is specified; default=5.
{p_end}
{synopt:{opt foldvar(varlist)}}
Integer variable(s) with user-specified cross-fitting folds; one foldvar per resample.
{p_end}
{synopt:{opt norandom}}
Use observations in existing order instead of randomizing before splitting into folds;
if multiple resamples, applies to first resample only;
ignored if user-defined fold variables are provided in {opt foldvar(varlist)}.
{p_end}
{synopt:{opt reps(integer)}}
Number of resampling iterations, i.e., how often the cross-fitting procedure is
repeated on randomly generated folds;
ignored if {opt foldvar(varlist)} is specified;
default=1.
{p_end}
{synopt:{opt vtype(string)}}
Variable type of the variable to be created. Defaults to {it:double}. 
{it:none} can be used to leave the type field blank.
{p_end}


{marker summary}{...}
{title:Summary}

{pstd}
{opt crossfit} fits a supervised machine learner on K-1 folds
and returns the out-of-sample predicted values for the holdout fold.
This process is repeated so that each fold serves once as the holdout fold
for which predictions are created.
At the end of the cross-fitting, a full set of predictions is available
in the new variable specified by the {opt generate} option.
The "supervised machine learner" can be any Stata estimator
that supports standard postestimation prediction.
{p_end}

{pstd}
{opt crossfit}'s default is to generate a single random split into folds.
This can be overridden by specifying user-defined fold variables,
or by the {opt norandom} option (indicating that the split use the data in the existing order).
{p_end}

{pstd}
{opt crossfit} allows multiple resampling,
meaning that the procedure is applied repeatedly
using multiple fold variables that indicate different fold splits.
This can be done via the {opt reps} option,
or by providing multiple user-defined fold variables.
The resample number is appended to the generated predictions.
{p_end}

{pstd}
The output of {opt crossfit} can be seen as the intermediate step
of standard K-fold cross-validation.
In a typical cross-validation exercise, a search is conducted across a range of specifications (e.g. values for a tuning parameter).
The prediction errors for the holdout folds are assembled for each specification,
and the specification with the best prediction performance (e.g. smallest mean squared prediction error) is chosen.
A simple example of how to use {opt crossfit} to do this is below.
{p_end}

{pstd}
{opt crossfit} has integrated support for {opt pystacked} (see the help for {help pystacked} if installed).
{help pystacked} is a front-end for the {browse "https://scikit-learn.org/stable/index.html":scikit-learn}
implementation of stacking regression.
Stacking is a way of combining multiple supervised
machine learners (the "base" or "level-0" learners) into
an ensemble or "meta" learner.
When used in conjunction with {opt crossfit}, the predictions of the {help pystacked} base learners
are generated along with the ensemble predicted values.
{p_end}


{marker compatibility}{...}
{title:Compatible programs}

{pstd} 
See {help ddml##compatibility:here}.


{marker examples}{...}
{title:Examples}

{input}. use http://fmwww.bc.edu/repec/bocode/j/jtpa.dta, clear
{txt}
{input}. global X sex age married black hispanic
{txt}
{input}. set seed 42
{txt}

{pstd}Note that the variable created is called yhat_1 because the number of resamples defaults to 1.{p_end}

{input}. crossfit, estring(reg earnings $X) gen(yhat) kfolds(3)
{res}{txt}Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}
{input}. sum earnings yhat_1

{txt}    Variable {c |}        Obs        Mean    Std. dev.       Min        Max
{hline 13}{c +}{hline 57}
{space 4}earnings {c |}{res}     11,204    15815.29    16767.05          0     155760
{txt}{space 6}yhat_1 {c |}{res}     11,204     15812.1    3774.255   6123.432   23867.51
{txt}

{pstd}As above but using 5 resamples.{p_end}

{input}. crossfit, estring(reg earnings $X) gen(yhat) kfolds(3) reps(5)
{res}{txt}Resample 1...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 2...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 3...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 4...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 5...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}
{input}. sum earnings yhat*

{txt}    Variable {c |}        Obs        Mean    Std. dev.       Min        Max
{hline 13}{c +}{hline 57}
{space 4}earnings {c |}{res}     11,204    15815.29    16767.05          0     155760
{txt}{space 6}yhat_1 {c |}{res}     11,204    15817.76    3772.003   5660.687   24048.23
{txt}{space 6}yhat_2 {c |}{res}     11,204    15811.39    3776.824   5764.323   24005.49
{txt}{space 6}yhat_3 {c |}{res}     11,204    15818.01    3777.579   5495.782      24012
{txt}{space 6}yhat_4 {c |}{res}     11,204    15811.81    3777.137   5751.744   23775.22
{txt}{hline 13}{c +}{hline 57}
{space 6}yhat_5 {c |}{res}     11,204    15817.15     3777.96    6044.12   23928.68
{txt}

{pstd}As above but using {help pystacked}.
The default base learners are OLS, CV-lasso and gradient boosting.{p_end}

{input}. crossfit, estring(pystacked earnings $X) gen(yhat) kfolds(3) reps(5)
{res}{txt}calling pystacked on full sample with noestimate option...
N={res}11204
{txt}number of learners = {res}3
{txt}Base learners: {res}ols lassocv gradboost 
{txt}Resample 1...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 2...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 3...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 4...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 5...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}
{input}. sum earnings yhat*

{txt}    Variable {c |}        Obs        Mean    Std. dev.       Min        Max
{hline 13}{c +}{hline 57}
{space 4}earnings {c |}{res}     11,204    15815.29    16767.05          0     155760
{txt}{space 3}yhat_L1_1 {c |}{res}     11,204    15814.67    3771.158   6154.772   23792.97
{txt}{space 3}yhat_L2_1 {c |}{res}     11,204    15814.67    3765.064   6180.004   23777.85
{txt}{space 3}yhat_L3_1 {c |}{res}     11,204    15816.24    4484.243   1665.902   28229.47
{txt}{space 6}yhat_1 {c |}{res}     11,204    15816.54    4153.827   3455.456   26655.86
{txt}{hline 13}{c +}{hline 57}
{space 3}yhat_L1_2 {c |}{res}     11,204    15814.28     3773.44   5357.084   23895.38
{txt}{space 3}yhat_L2_2 {c |}{res}     11,204    15814.28    3767.491   5381.269   23879.07
{txt}{space 3}yhat_L3_2 {c |}{res}     11,204    15813.21    4482.175  -3728.535   29254.65
{txt}{space 6}yhat_2 {c |}{res}     11,204    15812.07    4175.589  -764.1271   26819.45
{txt}{space 3}yhat_L1_3 {c |}{res}     11,204    15816.28    3782.185   5912.759    23893.9
{txt}{hline 13}{c +}{hline 57}
{space 3}yhat_L2_3 {c |}{res}     11,204     15816.3    3774.708   5960.517   23865.76
{txt}{space 3}yhat_L3_3 {c |}{res}     11,204     15817.7    4492.836   1363.014   34460.92
{txt}{space 6}yhat_3 {c |}{res}     11,204    15817.87    4204.361    3435.11   31376.77
{txt}{space 3}yhat_L1_4 {c |}{res}     11,204    15815.53     3776.09   5301.554      24088
{txt}{space 3}yhat_L2_4 {c |}{res}     11,204    15815.53    3770.267   5327.434   24072.52
{txt}{hline 13}{c +}{hline 57}
{space 3}yhat_L3_4 {c |}{res}     11,204    15821.58     4484.34  -1822.032   31067.63
{txt}{space 6}yhat_4 {c |}{res}     11,204     15819.3    4159.096   1104.146   28815.89
{txt}{space 3}yhat_L1_5 {c |}{res}     11,204    15816.62    3781.457   6159.217   23814.33
{txt}{space 3}yhat_L2_5 {c |}{res}     11,204    15816.62    3775.649   6184.389   23798.95
{txt}{space 3}yhat_L3_5 {c |}{res}     11,204    15804.12    4469.694   818.7079    29594.9
{txt}{hline 13}{c +}{hline 57}
{space 6}yhat_5 {c |}{res}     11,204    15806.93    4184.026   2440.185   27196.09
{txt}

{pstd}A simple example of 3-fold cross-validation with 5 resamples using {opt crossfit}.
{input}. ssc install lassopack
{txt}checking {hilite:lassopack} consistency and verifying not already installed...
{smcl}
all files already exist and are up to date.
{txt}
We estimate using the following values of the lambda parameter: 2000, 1000, 500, 250.
Each time we call {opt crossfit} to obtain the predicted values.
These could be used after cross-fitting to calculate the MSPE (mean squared prediction error),
but the MSPE is one of the returned results of {opt crossfit} so we just report that.
The specification that minimizes the MSPE for all 5 resamples is lambda=250.
{p_end}

{input}. crossfit, estring(lasso2 earnings $X, lglmnet lambda(2000)) gen(yhat2000) kfolds(3) reps(5)
{res}{txt}Resample 1...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 2...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 3...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 4...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 5...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}
{input}. mat list r(mse_list)
{res}
{txt}r(mse_list)[5,1]
                   c1
resample_1 {res} 2.758e+08
{txt}resample_2 {res} 2.759e+08
{txt}resample_3 {res} 2.759e+08
{txt}resample_4 {res} 2.759e+08
{txt}resample_5 {res} 2.758e+08
{reset}{txt}
{input}. crossfit, estring(lasso2 earnings $X, lglmnet lambda(1000)) gen(yhat1000) kfolds(3) reps(5)
{res}{txt}Resample 1...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 2...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 3...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 4...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 5...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}
{input}. mat list r(mse_list)
{res}
{txt}r(mse_list)[5,1]
                   c1
resample_1 {res} 2.710e+08
{txt}resample_2 {res} 2.710e+08
{txt}resample_3 {res} 2.709e+08
{txt}resample_4 {res} 2.710e+08
{txt}resample_5 {res} 2.709e+08
{reset}{txt}
{input}. crossfit, estring(lasso2 earnings $X, lglmnet lambda(500)) gen(yhat500) kfolds(3) reps(5)
{res}{txt}Resample 1...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 2...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 3...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 4...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 5...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}
{input}. mat list r(mse_list)
{res}
{txt}r(mse_list)[5,1]
                   c1
resample_1 {res} 2.684e+08
{txt}resample_2 {res} 2.686e+08
{txt}resample_3 {res} 2.686e+08
{txt}resample_4 {res} 2.685e+08
{txt}resample_5 {res} 2.684e+08
{reset}{txt}
{input}. crossfit, estring(lasso2 earnings $X, lglmnet lambda(250)) gen(yhat250) kfolds(3) reps(5)
{res}{txt}Resample 1...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 2...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 3...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 4...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}Resample 5...
Cross-fitting fold 1 {res}{txt}2 {res}{txt}3 {res}{txt}...completed cross-fitting
{res}{txt}
{input}. mat list r(mse_list)
{res}
{txt}r(mse_list)[5,1]
                   c1
resample_1 {res} 2.675e+08
{txt}resample_2 {res} 2.676e+08
{txt}resample_3 {res} 2.679e+08
{txt}resample_4 {res} 2.676e+08
{txt}resample_5 {res} 2.673e+08
{reset}{txt}

{pstd}When used as a standalone program, {opt crossfit} leaves behind in Mata a eStruct ("equation struct") called "crossfit".
This object contains information about the estimation, stored on associative arrays.
The utility {help ddml extract} can be used to extract this information.
The example below shows how to list the AA keys
and how to extract the {help pystacked} stacking weights for resample 2.
Rows are base learners; columns are the weights for each learner.{p_end}

{input}. mata: mata desc crossfit

      {txt}# bytes   type                        name and extent
{hline 79}
{res}            8   {txt}struct scalar               {res}crossfit
{txt}{hline 79}
{txt}
{input}. ddml extract, ename(crossfit) keys
{res}{txt}AA keys for eqn crossfit.lrnAA:
                  1             2
     {c TLC}{hline 29}{c TRC}
   1 {c |}  {res}        opt             1{txt}  {c |}
   2 {c |}  {res}        opt             2{txt}  {c |}
   3 {c |}  {res}        opt             3{txt}  {c |}
   4 {c |}  {res}        opt             4{txt}  {c |}
   5 {c |}  {res}        opt             5{txt}  {c |}
   6 {c |}  {res}    yhat250      est_main{txt}  {c |}
   7 {c |}  {res}    yhat250   est_options{txt}  {c |}
   8 {c |}  {res}    yhat250       estring{txt}  {c |}
   9 {c |}  {res}    yhat250       predopt{txt}  {c |}
  10 {c |}  {res}    yhat250         vtype{txt}  {c |}
     {c BLC}{hline 29}{c BRC}
{txt}AA keys for eqn crossfit.resAA:
                1           2           3
     {c TLC}{hline 37}{c TRC}
   1 {c |}  {res}  yhat250         MSE           1{txt}  {c |}
   2 {c |}  {res}  yhat250         MSE           2{txt}  {c |}
   3 {c |}  {res}  yhat250         MSE           3{txt}  {c |}
   4 {c |}  {res}  yhat250         MSE           4{txt}  {c |}
   5 {c |}  {res}  yhat250         MSE           5{txt}  {c |}
   6 {c |}  {res}  yhat250   MSE_folds           1{txt}  {c |}
   7 {c |}  {res}  yhat250   MSE_folds           2{txt}  {c |}
   8 {c |}  {res}  yhat250   MSE_folds           3{txt}  {c |}
   9 {c |}  {res}  yhat250   MSE_folds           4{txt}  {c |}
  10 {c |}  {res}  yhat250   MSE_folds           5{txt}  {c |}
  11 {c |}  {res}  yhat250           N           1{txt}  {c |}
  12 {c |}  {res}  yhat250           N           2{txt}  {c |}
  13 {c |}  {res}  yhat250           N           3{txt}  {c |}
  14 {c |}  {res}  yhat250           N           4{txt}  {c |}
  15 {c |}  {res}  yhat250           N           5{txt}  {c |}
  16 {c |}  {res}  yhat250     N_folds           1{txt}  {c |}
  17 {c |}  {res}  yhat250     N_folds           2{txt}  {c |}
  18 {c |}  {res}  yhat250     N_folds           3{txt}  {c |}
  19 {c |}  {res}  yhat250     N_folds           4{txt}  {c |}
  20 {c |}  {res}  yhat250     N_folds           5{txt}  {c |}
     {c BLC}{hline 37}{c BRC}
{res}{txt}
{input}. ddml extract, ename(crossfit) key1(yhat) key2(stack_base_est)
{res}  0x0
{txt}
{input}. ddml extract, ename(crossfit) key1(yhat) key2(stack_weights) key3(2)
{res}  0x0
{txt}


{marker results}{title:Saved results}

{p}{opt crossfit} saves the following results in {cmd:r()}:

Scalars
{col 4}{opt r(N)}{col 25}Number of observations.
{col 4}{opt r(mse)}{col 25}Mean squared prediction error in the last resample.

Macros
{col 4}{opt r(cmd_list)}{col 25}Estimation command

Matrices
{col 4}{opt r(N_list)}{col 25}Sample size; rows are resamples.
{col 4}{opt r(mse_list)}{col 25}MSPE; rows are resamples.
{col 4}{opt r(N_folds_list)}{col 25}Sample size by fold; rows are resamples.
{col 4}{opt r(mse_folds_list)}{col 25}MSPE by fold; rows are resamples.


{marker references}{title:References}

{phang}
Ahrens, A., Hansen, C.B. and M.E. Schaffer. 2020.
lassopack: model selection and prediction with regularized regression in Stata.
{it:The Stata Journal}, 20(1):176-235.
{browse "https://journals.sagepub.com/doi/abs/10.1177/1536867X20909697"}.
Working paper version: {browse "https://arxiv.org/abs/1901.05397"}.{p_end}

{phang}
Chernozhukov, V., Chetverikov, D., Demirer, M., 
Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018), 
Double/debiased machine learning for 
treatment and structural parameters. 
{it:The Econometrics Journal}, 21: C1-C68. {browse "https://doi.org/10.1111/ectj.12097"}


{marker authors}{title:Authors}

{pstd}
Achim Ahrens, Public Policy Group, ETH Zurich, Switzerland  {break}
achim.ahrens@gess.ethz.ch

{pstd}
Christian B. Hansen, University of Chicago, USA {break}
Christian.Hansen@chicagobooth.edu

{pstd}
Mark E Schaffer, Heriot-Watt University, UK {break}
m.e.schaffer@hw.ac.uk   

{pstd}
Thomas Wiemann, University of Chicago, USA {break}
wiemann@uchicago.edu


{title:Also see (if installed)}

{pstd}
Help: {help ddml}, {help qddml}, {help pystacked}, {help lasso2}, {help cvlasso}.{p_end}
{smcl}
{txt}{sf}{ul off}